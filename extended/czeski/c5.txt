
Hlavní menu

WikipedieWikipedie: Otevřená encyklopedie
Hledat na Wikipedii
Hledat
Vytvoření účtu
Přihlášení

Osobní nástroje
Obsah skrýt
(úvod)
Původ pojmu
Definice
Informace ve vědě
Informace a jazyk
Zdroj informace
Typy poznání
Podstata informace
Kvantifikace
Vlastní informace
Informace - odstranění neurčitosti
Vzájemná informace
Mezi reálným světem a informací
Informace a interpretace
Informace a uspořádanost
Zpracování informace
Přenos informace – komunikace
Přepnout podsekci Přenos informace – komunikace
Výklad pojmu informace
Vlastnosti informace
Některé souvislosti
Šíření a uchovávání informace
Zpracování informací
Měření informace
Informace v dalších vědách
Odkazy
Přepnout podsekci Odkazy
Informace

Článek
Diskuse
Číst
Editovat
Editovat zdroj
Zobrazit historii

Nástroje
Informace (z lat. in-formatio, utváření, ztvárnění) je velmi široký, mnohoznačný pojem, který se užívá v různých významech. V nejobecnějším smyslu je informace chápána jako údaj o dění v reálném světě. Informace snižuje nejistotu (neurčitost znalosti) člověka o dění v jisté části reálného světa. Cestou ke snížení té nejistoty je poznání tj. získání informace o oné části reálného světa. Množství informace lze charakterizovat tím, jak se jejím získáním změnila míra nejistoty příjemce.[1] O měření množství informace je pojednáno níže v odst. Kvantifikace.

V běžné řeči:
informace jako vědění, které lze předávat, jako obsah zprávy či sdělení;
informace (plurál) – místo, kde se lze o něčem informovat.
V informatice se operuje s řetězci znaků (Hartley), které lze vysílat, přijímat, uchovávat a zpracovávat technickými prostředky. Příjem znaku (zprávy) dodaného sdělovacím zařízením, snižuje nejistotu příjemce o tom, který ze znaků mohl obdržet. Množství informace je rozdíl mezi neurčitostí příjemcovy informace před a po přijetí zprávy o obdrženém znaku.
Původ pojmu
Latinské informatio znamená původně vtištění formy či tvaru, utváření. Slovo se však metaforicky používalo pro „utváření mysli“ – učení a vzdělávání – a odtud dalším významovým posunem mohlo znamenat i sdělení, zprávu. Odtud pochází slovo informátor, doložené od 16. století. Zkrácený tvar info vznikl až ve 20. století v angličtině. Vědecký zájem o informaci začíná také až ve 20. století v souvislosti s elektronickou komunikací a počítači a na druhé straně se studiem obecného uspořádání, struktur a kódů.

Definice
Cesta k definici: Pojem může být definován několika způsoby, jedním z nich je vytyčení určujícími rysy – invariantními příznaky entity (reálného či abstraktního světa) ke které odkazuje. Ty z invariantních příznaků, které jsou sdělitelné, lze použít pro vytyčení-definici pojmu. Invariantní příznaky pojmu informace jsou uvedeny v následujících odstavcích, a jejich souhrn tvořící definici je:

Informace:

Je abstraktní entita (myšlenkový konstrukt).
Snižuje nejistotu (neurčitost znalosti) člověka o dění v jisté části reálného světa.
Jejím zdrojem je poznání.
Lze ji ukládat, transportovat a zpracovávat k získání jiné, pro daný účel požadované informace.
Vždy je vázána na jazyk, který s ní umožňuje provádět výše uvedené operace.
Člověku umožňuje jiný pohled na dění v reálném světě (vidění jiných jevů a souvislostí), než umožňuje působení interakcí, jež je klíčové pro přírodní i technické vědy.
Informace ve vědě
Informace je abstraktní entita,[2][3] pro její uchopení – pro archivaci (pamatování), a tak i šíření (sdělování), nebo pro její zpracování – je nutný nějaký „kontejner“, říká se mu jazyk, do něj je vtisknuta. Jazyk je abstraktní struktura (řád mezi vhodnými primitivy),[4][5] která pro aplikaci musí být materializována vhodnou strukturou hmoty či energie. Informace je uchopovací nástroj člověka (podobně jako mnohé jiné, např. souřadnice prostoru či jeho metrika, kauzalita a další). Pojem informace má být odpovědí na otázku, co to (jakou entitu) získáváme poznáváním (reálného světa), co ukládáme, když pamatujeme, co transportujeme, když sdělujeme. Říkáme, že informaci, ale ve skutečnosti jen ten (přírodou zapečetěný) kontejner – jazykové konstrukty.

Obvykle rozlišujeme elementární a komplexnější informaci. Elementárním říkáme data (ze smyslových orgánů člověka, ze snímačů, čidel a měřících zařízení), komplexnějším, vybudovaným na rozpoznaných souvislostech mezi daty, říkáme znalosti.

Informace a jazyk
Důvody proč informace vyžaduje kontejner – jazyk jsou dva:

Princip lidské komunikace, kterým člověka obdařila příroda, neboť jazyk je v ní zásadní součástí. V tomto případě jazykové konstrukty – formy (slova, věty) slouží jako pointery (ukazatelé) do vnitropsychického kognitivního modelu příjemce zprávy, umožňující mu přiřadit přijatému jazykovému konstruktu – formě (slovu, větě) informaci, kterou považuje za nejvhodnější. Toto přiřazení se nazývá konotace a znamená, že příjemce zprávy jazykové formě porozuměl, přiřadil ji význam. (toto je jen část potřebná pro vysvětlení funkce jazyka, celý princip lidské komunikace viz níže: Přenos informace – komunikace)
Archivaci (pamatování), šíření i zpracování informace nelze provést jinak, než využitím procesů reálného (materiálního) světa. Má-li se v reálném světě něco dít, musejí probíhat interakce. Znamená to, že informaci, která je abstraktní entitou (není aktérem), je třeba včlenit do reálného světa interakcí, učinit ji aktérem v něm. Lze to pouze prostřednictvím jazyka (informace jako abstraktní entita je jinak neuchopitelná), který je nositelem informace výše popsaným způsobem (tedy tak, že jeho konstrukty slouží jako pointery). Proto všechny výše zmíněné operace s informací, jsou ve skutečnosti operacemi s kontejnerem – jazykem.
Jazyk má vzhledem k informaci funkci prostředníka, který jí umožňuje (propůjčuje ji své vlastnosti) přestoupit z jednoho prostředí do jiného prostředí, z lidské psýchy (biochemických a elektrochemických interakcí probíhajících v lidském mozku) do vnějšího reálného světa interakcí a obráceně. Takovému prostředníkovi se někdy říká vehikulum. Takto označuje vnější sdělovací jazyk např. sémiotik Charles William Morris.[6] Jazyk materializací se dostávající do světa interakcí, může být i vykonavatelem příkazu – informace (např. vyvolá pohyb ramena robota), aspoň tak to někdy chceme vidět, avšak fyzika to vidí jako řetěz interakcí.

Tak i Shannonova entropie se týká jazyka (i když v tomto případě na jeho abstraktní úrovni), nikoli informace,[7] a je mírou neurčitosti výskytu znaků-jazykových konstruktů ve zprávě.[8] Jaké informace ony jazykové konstrukty vyvolávají v psýše příjemce zprávy, je v tomto případě mimo hranice diskursu. Někteří autoři mylně tvrdí, že Shannon ke své entropii došel z myšlenky stavící na tom, že některá informace je pro příjemce překvapivější či důležitější než jiná, a tak je lze kvantitativně ohodnotit. Avšak příjemce zprávy nelze vyzpovídat takovým způsobem, aby získané výsledky bylo možno použít pro přestup (použití) do exaktního světa (matematiky), ve kterém Shannon formuloval svoji entropii. Tento problém přenosu dat z lidské psýchy do exaktního světa je blíže popsán na fuzzy logika. Shannon se dostal tak blízko k entitě informace, jak to umožňuje exaktní věda. V Shannonově přístupu lze nalézt volnou paralelu s výše uvedenou překvapivostí obdržené informace. A to při jeho zkoumání informace obsažené v pravděpodobnostním rozdělení (viz Kvantifikace), kdy za překvapivější mohou být považovány výskyty jevů méně pravděpodobných, tedy méně očekávaných.

Je vidět jistá výjimečnost pojmu informace, neboť v podstatě, i to že informace existuje v lidské psýše, je lidská tvůrčí představa, ve skutečnosti je to výsledek biochemických a elektrochemických interakcí probíhajících v lidském mozku zajišťujících pamatování, sdělování i zpracování toho, čemu říkáme informace. Informace je tedy skutečně jen jiné vidění reálného světa, než vidění interakcí, jiná interpretace. Je lhostejné, jedná-li se o dění v lidském mozku či vně, v jiné části reálného světa. Je pro nás vžité, přijatelné a užitečné, že v mozku pamatujeme i zpracováváme informaci, tedy myslíme. Je tak např. možno studovat zákonitosti myšlení. Informace je pro člověka důležitý uchopovací nástroj – pojem.

Zdroj informace
Primárním zdrojem informace je poznání. Je to zrození informace, ta před tím neexistovala. Poznání je však možné jen se současným záznamem poznaného (informace) do jazyka, i v lidské psýše se předpokládají vnitropsychické jazyky. Poznání neposkytne samotnou informaci, ale její jazykové uchopení. Proto pro každé poznání musí být k dispozici adekvátní jazyk, buď se o to postarala příroda, nebo ho vytvořil člověk. Pokud takový jazyk neexistuje, musí být předem vytvořen viz Exaktní věda, Věda, a musí být „na míru“ poznání, říká se adekvátní. Budování nových matematických disciplín, které nabízejí nové jazyky, jsou často impulzem pro nové možnosti poznání, např. zejména ve fyzice částic.

Typy poznání
Přirozené – filtrem poznání je vágnost – získaná informace s inherentní vnitřní vágností. Jazyk je přirozený (neformální), jen ten je schopen, díky své vágní, subjektivní a emocionální interpretaci (říkáme jí konotace) převzít informaci (svázanou s inherentní vágností) získanou přirozeným poznáním.
Umělé, exaktní (Newton) – diskrétní filtr poznání – získaná informace s nulovou vnitřní vágností viz Exaktní věda, Věda. Jazyk musí být formální – matematika, formální logika, programovací jazyky. Jen takový jazyk je schopen reprezentovat informaci s nulovou vnitřní vágností interpretace, získanou exaktním poznáním. Výsledkem poznání je informace, jsou to rozpoznané přírodní zákony zapsané matematickým jazykem, jako vztahy mezi veličinami. Podle použitého jazyka se mu říká matematický model.
Poznamenejme, že se používá ještě poněkud jinak chápaný pojem zdroj informace ve schématu, kterému se říká Sdělovací kanál. Tam je zdroj informace jakékoli zvolené referenční místo, které pro daný úkol považujeme za to, ze kterého se informace šíří bez ohledu na způsob jejího získání. Může to být třeba místo na konstrukci letadla, ze kterého se radarový paprsek někam odráží, a pro danou úlohu řešenou z hlediska přenosu informace, je podstatné, co se děje od tohoto místa někam dále. Obvykle je to cesta odražené části paprsku do přijímací antény, přijímače a dále do řetězce zpracování této informace.

Podstata informace
Podstata informace je jedna, má však dvě výše uvedené nezaměnitelné modifikace (typy) dané vazbou s inherentní vnitřní vágností (neurčitostí). Informace získaná poznáním obohatí kognitivní (znalostní) model člověka, zmenší jeho pochybnosti o dění v jisté části reálného světa, sníží tak jeho nejistotu, původně strach z neznáma, z hladové šelmy v buši, zvýší jeho šanci na přežití. Je to ale jiný druh nejistoty, než je vágnost – nástroj (filtr) přirozeného lidského poznání. Souvisí s mírou poodhalení té věčně zamlžené „pravdy“ hledané člověkem. Exaktní svět je obvykle rozlišuje způsobem reprezentace: vágnost fuzzy nástroji, nejistotu poznání stochastickými nástroji. Hypotéza charakterizující informaci jako entitu odstraňující nejistotu o dění v reálném světě, se stala dominantní, a je na ni postavena teorie informace.

Informace je abstraktní entita (kdybychom ji byli schopni měřit, byla by veličinou) světa kognitivních (znalostních) modelů reálného světa tvořených člověkem. Informace vznikne poznáním (je to její zdroj), pokud by člověk neměl schopnost poznání, nebyla by informace. Lidské abstraktní konstrukce mimo lidský svět neexistují, některé z nich lze uchopit (formulovat) vnějším sdělovacím jazykem, příkladem je matematika, geometrie, architektonické náčrty, skulptury a jiné jazykové útvary.

Kvantifikace
Tento odstavec ukazuje cestu od prvních myšlenek (Hartley), jak kvantifikovat informaci, po Shannonovo epochální stanovení množství informace jako míry odstranění neurčitosti. Tato cesta byla otevřena tak, že místo nástrojově neuchopitelné informace se používají její nástrojově uchopitelní nositelé – znaky (jazykové konstrukty). Od jejich úlohy nositelů „vtištěné“ informace je však zcela obhlédnuto, a sleduje se pouze řád, ve kterém se znaky objevují jejich příjemci. V tomto řádu je obsažena (je jím určena) předmětná informace, která je nadále ústředním pojmem (formální) informatiky, a tak předmětem zkoumání.[9] Podstatné je, že při této záměně je zachován princip, že příchody znaků příjemci snižují neurčitost toho, jaký znak by mohl přijít, podobně jako získání informace příjemci snižuje neurčitost jeho znalosti o dění v jisté části reálného světa.

Z oboru techniky sdělování (přenosu) informace vyšel první pokus (Ralph Hartley 1928) nalézt metodu měření množství přenesené informace,[10] tak i rychlost, jako množství informace přenesené za jednotku času. Hartley přišel s myšlenkou, kdy měření množství informace je založeno na předpokladu (modelu), že odesílatel zprávy má k dispozici (konečnou) množinu znaků např. anglickou abecedu, ze kterých vytváří posloupnosti, a ty odesílá. Shannon za podpory matematika Warrena Weavera rozšířil Hartleho model myšlenkou, že pro účely kvantifikace informace mohou být sekvence místo operátora (odesílatele) generovány náhodným jevem, aniž by se tím porušil princip snižování neurčitosti při jejich příjmu. Řád, jímž je informace určena, je pak rozdělení pravděpodobnosti onoho náhodného jevu. V takovém pojetí je zároveň k dispozici nástroj umožňující vyjádřit (Shannonovu) ideu, že získání informace znamená snížení neurčitosti: Informace je údaj o tom, že nastal jev z množiny možných náhodných jevů, což příjemci sníží neurčitost znalosti o tom, který z jevů mohl nastat.

Rozhodnutí, že místo nástrojově neuchopitelné informace se budou používat nástrojově uchopitelné jazykové konstrukty, tak zachovává princip, že se jedná o snižování neurčitosti znalostí příjemce:

Získání informace snižuje nejistotu znalostí příjemce o dění v jisté části reálného světa. (Uznávaná hypotéza o podstatě informace i lidské vnímání podstaty informace)
Příjem znaku (jazykové konstrukce) dodaného sdělovacím zařízením, snižuje nejistotu příjemce o tom, který znak mohl obdržet. A poněkud abstraktněji: Informace je údaj o tom, že nastal jev z množiny možných náhodných jevů, což příjemci sníží neurčitost znalosti o tom, který z jevů mohl nastat.
A tato poslední věta je již shannonovské pojetí, kdy Shannon (ve své práci[11]) z něj vychází jeho formalizovaným (matematickým) vyjádřením (Shannon citace):

Předpokládejme, že máme množinu možných událostí, jejichž pravděpodobnost výskytu je

p1, p2, ... pn.
Tyto pravděpodobnosti jsou známy, ale to je vše, co víme o tom, která událost nastane. Můžeme najít měřítko toho, kolik „volby“ je zahrnuto ve výběru události nebo jak nejistí jsme ohledně výsledku?

Pokud taková míra existuje, řekněme veličina H (p1, p2,…, pn), je rozumné požadovat od ní následující vlastnosti:

H by mělo být spojité v pi.
Pokud jsou všechna pi stejná, pi = 1 /n, pak H by měla být monotónní rostoucí funkce proměnné n. U stejně pravděpodobných událostí existuje větší výběr nebo nejistota, při rostoucím počtu možných událostí.
Pokud je volba rozdělena na dvě po sobě jdoucí volby, původní H by měl být vážený součet jednotlivých hodnot H.
Shannon dokazuje, že jediná funkce, která výše uvedené vlastnosti splňuje, je tvaru

�
=
−
�
∑
�
=
1
�
�
�
log
⁡
(
�
�
)
{\displaystyle H=-K\sum _{i=1}^{n}p_{i}\log(p_{i})}
kde K je kladná konstanta, jejíž hodnota určuje měřítko jednotek (pro měření v řádu jednotek bitů se klade K = 1), ve kterých se veličina H udává, a log je logaritmická funkce, jejíž základ se položí roven 2, pokud má H být udáváno v bitech.

Shannon formuloval problém nalezení míry informace pro konečný soubor pravděpodobností

p1, p2,... pn
a Shannonovým cílem bylo zjistit, kolik neurčitosti je obsaženo v rozdělení pravděpodobnosti takového souboru. Pak veličinu H interpretoval jako míru informace, výběru a neurčitosti (measures of information, choice and uncertainty). Několik výkladů významu Shannonova vztahu pro H, je uvedeno např. v práci[12], a též na Informační entropie.

Doplnění:

Důvodem, proč Shannon svoji úlohu, kdy hledal míru informace (kvantifikaci) formuloval v exaktním světě, tedy nástroji matematiky, je, že kvantifikaci lze formulovat pouze v exaktním světě, neboť ve světě vágnosti lidské psýchy lze cokoliv kvantifikovat pouze vágně, subjektivně a emocionálně.
Typ informace, o kterou se jedná v pojetí Hartleyho a následně Shannona, je určen tím, že je to informace reprezentovaná nástroji exaktního světa zde jazykem matematiky. Není to obecná informace, ale informace reprezentovaná formálním jazykem, takže to nemůže být informace jiného typu, než exaktní viz Jazyk (lingvistika), Věda. Má tedy buď exaktní interpretaci (do reálného světa viz Exaktní), nebo nemá žádnou, což je v matematice běžné. Nemůže to tedy být „lidská“ informace s inherentní vnitřní vágností, tu je schopna interpretovat pouze lidská psýcha viz konotace. Lidská informace s inherentní vnitřní vágností, a exaktní informace s nulovou vnitřní vágností interpretace, mají společnou podstatu, snižují neurčitost v poznání příjemce a spolu tvoří veškerou informaci, kterou člověk používá.
Jak na Shannonovu práci reagují lingvisté, je zřejmé z jejich stesků, že Shannon, na jejich zájmy ve své práci zapomněl:[13] Předchozí odstavec jim napoví, že Shannonova míra informace se týká pouze exaktní informace, a že lidská inherentně vágní informace a její případné měření spadá do kompetence kognitivní psychologie.
Důsledkem rozhodnutí uvažovat jazykové konstrukty místo informace, je pro kvantifikaci informace ten, že pokud jisté pomyslné množství informace vtiskneme do dvou jazyků s různou strukturou znaků, bude množství změřené informace obsažené v každém z jazyků jiné. Toto je důvod, který způsobil hledání jazykových struktur, které jsou schopny přenést více (té pomyslné) informace. Je to soutěž mezi jazyky s různou strukturou z hlediska vtisknuté informace, avšak ve fyzice, chemii, ....jsou hlediska jiná, tam je to hledání jazyka, který je schopen popsat zkoumaný jev. Jazyková struktura se dá uchopit způsobem, kterému se říká kódování, a vyvinul se tak obor kódování, hledání kódů potřebných vlastností např. přenášejících maximální množství informace, odolných vůči chybě v přenosu znaků na úkor množství užitečné informace apod. Přesto že se tedy neměří informace, ale struktura jazyka vytyčená řádem jazykových znaků (primitiv), vyhovuje tento způsob měření množství informace kvantifikaci informační kapacity přenosového zařízení, kvantifikaci kapacity paměti zařízení i kvantifikaci rychlosti zpracování informace (ve skutečnosti jazykových konstruktů). Hartley vytyčil inspirující cestu, a nastalo myšlenkové pokračování. Na hypotéze, že informace působí snížení neurčitosti, dal Shannon základy teorii informace jako exaktní vědě (v článku: Claude Elwood Shannon, Warren Weaver: „A mathematical theory of communication“ v r. 1948) viz Informační entropie. Mnoho dalších vědců na ni navázalo.

Vlastní informace
Nechť X je množina n možných událostí xi, s pravděpodobnostmi výskytu (někdy se říká realizace nebo výběr) pi, a kde ∑ pi = 1.

Množství informace I(xi), které je v každé jednotlivé události xi, která má pravděpodobnost výskytu pi, lze stanovit jako funkci pravděpodobnosti výskytu té události takto (Hartlyho míra informace):

I(xi) = -log(pi), (0 < pi ≤ 1)
Pokud se za základ logaritmu zvolí 2, je hodnota I(xi) uvedena v bitech. Znaménko mínus před logaritmem je vloženo proto, že logaritmus pro uvedený interval argumentu (0 < pi ≤ 1) má zápornou funkční hodnotu, a množství informace I(xi) musí být kladná hodnota.

I(xi) se nazývá vlastní informace (ang. self-information), tj. množství informace, kterou přinese výskyt události xi. Vztah pro I(xi) matematicky demonstruje myšlenku, že čím je pravděpodobnost pi výskytu události xi menší (tedy vzácnější), tím její výskyt přináší větší množství informace. Případ pi = 0 (výskyt, který nemůže nastat) je nutno vyloučit, neboť by odpovídající informace byla nekonečná, což nelze připustit, model-vztah pro I(xi) by neodpovídal skutečnosti.

Pokud se ze všech I(xi) jichž je n, vytvoří vážený průměr, kde váhami jsou příslušné pravděpodobnosti pi, vznikne výše uvedený vztah pro Shannonovu entropii H. Entropie H je tak váženou střední hodnotou vlastních informací všech n jevů xi, a hodnota entropie H tak vyjadřuje průměrné množství vlastní informace na jeden výskyt pravděpodobnostního jevu xi.

Informace - odstranění neurčitosti
Jestliže jistá zpráva obsahuje K výskytů (např. znaků) pravděpodobnostního jevu X, a průměrné množství informace na jeden výskyt ve zprávě je dáno hodnotou entropie H (kdy se vážený průměr provede pro oněch K výskytů), pak ona zpráva přináší množství informace I = K * H. Tak neurčitost, která byla zprávou příjemci odstraněna, je rovna onomu množství informace I. To je případ, kdy se zachovává výše uvedené shannonovské odhlédnutí od reality, kdy výskyty jevů se považují za na sobě nezávislé, neboť se jedná o obecný případ, kdy nelze jejich závislost specifikovat. V konkrétních případech, např. kdy znaky náleží jistému přirozenému jazyku, se může navíc uvažovat, jak pravděpodobnost výskytu jistého znaku závisí (je podmíněna) na výskytu předchozího znaku (případně více předchozích znaků), jak je to onomu jazyku vlastní, což je pro řadu přirozených jazyků statisticky vypočítáno a publikováno [14]. Závislostí mezi znaky je pak ovlivněna i hodnota entropie H a tak i hodnota odstraněné neurčitosti, neboť závislost mezi výskytem znaků „napovídá“ (nese informaci), který znak by mohl následovat, což snižuje nepředvídatelnost těch jevů, a tak hodnotu entropie.Tuto situaci matematicky reprezentuje vztah

I(X;Y) = H(X) - H(X/Y)
uvedený v následujícím odstavci, kde nyní

I(X; Y) představuje průměrnou informaci na jeden výskyt (znak) z K výskytů obsažených ve zprávě,

H(X) je průměrná informace na jeden výskyt (znak) v případě, kdy znaky ve zprávě jsou nezávislé,

H(X/Y) je průměrná informace obsažená v pravděpodobnostní závislosti znaku na znaku předchozím.

Na hodnotě H (X/Y) se podílí informace, pocházející z jiného zdroje informace, než přijímaná zpráva. Je to informace lokální, získaná statistickou analýzou příslušného přirozeného jazyka, stanovující pravděpodobnost výskytu znaku, když už se objevil znak předchozí. Hodnota H (X/Y) představuje informaci, která je apriori už známa odjinud, tak neurčitost, která byla zprávou příjemci odstraněna, je rovna množství informace

I = K * I(X;Y),
které přichází kanálem ODESÍLATEL – PŘÍJEMCE. Celkové množství informace, které má příjemce k dispozici z obou zdrojů je

I = K * [I(X;Y) + H (X/Y)] = K * H(X).
Princip sdělování, kdy kanálem prochází minimum informace a větší kvantum informace má příjemce k dispozici lokálně (u sebe), lze realizovat tak, že kanálem procházejí pouze pointery (ukazatelé) do báze znalostí příjemce, kde se aktivuje (vybere) odpovídající informace. Tohoto principu použila příroda v lidské komunikaci (viz Konotace / komunikace), kde konstrukty sdělovacího jazyka (slova, věty) slouží jako pointery do lidské psýchy, disponující znalostmi (vnitro psychický kognitivní model) odpovídajícími přijímaným jazykovým konstruktům.

Vzájemná informace
Vzájemná informace má v teorii informace řadu aplikací, např. v informačním kanálu (Shannonově schématu) se požaduje maximální možná vzájemná informace mezi výstupem a vstupem kanálu s ohledem na přítomnost šumu a s ohledem na šíři pásma kanálu.

Vzájemná informace (někdy též transinformace) I(X;Y) je množství informace, které je společné náhodným proměnným X a Y. Je měřítkem vzájemné statistické závislosti mezi dvěma náhodnými proměnnými X a Y. Obvykle se udává v bitech. Tuto veličinu definoval a analyzoval Claude Shannon ve své klíčové práci Matematická teorie komunikace[15], avšak nenazval ji „vzájemnou informací“. Tento termín později vytvořil Robert Fano [16]

Vzájemná (průměrná) informace I(X; Y) vypovídá o tom, kolik informace o X lze získat při znalosti Y, to znamená, jakou měrou znalost jedné z těchto proměnných snižuje nejistotu znalosti o té druhé. Vzájemná informace měří průměrné množství informace poskytnuté výskytem náhodných jevů xi ϵ X při statistické závislosti na pravděpodobnosti výskytu náhodných jevů yj ϵ Y.

Vzájemná informace dvou pravděpodobnostních jevů xi, yj je podle Hartlyho míry informace:

�
(
�
�
;
�
�
)
=
�
�
�
(
�
(
�
�
|
�
�
)
�
(
�
�
)
)
=
�
�
�
(
�
(
�
�
,
�
�
)
�
(
�
�
)
�
(
�
�
)
)
{\displaystyle I(x_{i};y_{j})=log({\frac {p(x_{i}|y_{j})}{p(x_{i})}})=log({\frac {p(x_{i},y_{j})}{p(x_{i})p(y_{j})}})},
kde

p(xi) a p(yj) jsou (marginální) pravděpodobnosti jevů xi a yj.
p(xi | yj) je podmíněná pravděpodobnost jevů xi a yj, tj. pravděpodobnost jevu xi při realizaci (vyskytnutí se) jevu yj, tedy, když už je jev yj znám.
p (xi,yj) je sdružená pravděpodobnost jevů xi a yj, tj. pravděpodobnost současného výskytu jevů xi a yj.
Průměrná vzájemná informace měří váženou průměrnou vzájemnou informaci, která se vyskytuje mezi hodnotami xi, yj dvou náhodných proměnných. Váhou pro společný výskyt jevů xi; yj je pravděpodobnost jejich společného výskytu p (xi,yj). Průměrnou vzájemnou informaci dvou diskrétních náhodných proměnných X a Y lze pak definovat:

�
(
�
;
�
)
=
∑
�
=
1
�
�
(
�
�
)
∑
�
=
1
�
�
(
�
�
|
�
�
)
�
�
�
(
�
(
�
�
|
�
�
)
�
(
�
�
)
)
=
∑
�
=
1
�
∑
�
=
1
�
�
(
�
�
,
�
�
)
�
�
�
(
�
(
�
�
,
�
�
)
�
(
�
�
)
�
(
�
�
)
)
{\displaystyle I(X;Y)=\sum _{j=1}^{m}p(y_{j})\sum _{i=1}^{n}p(x_{i}|y_{j})log({\frac {p(x_{i}|y_{j})}{p(x_{i})}})=\sum _{i=1}^{n}\sum _{j=1}^{m}p(x_{i},y_{j})log({\frac {p(x_{i},y_{j})}{p(x_{i})p(y_{j})}})}
Definice průměrné vzájemné informace v poněkud odlišné interpretaci:

Definice z teorie informace[17]:

Informaci I(X;Y) ve zprávě Y o zprávě X zdroje
(
�
,
�
)
∼
(
�
×
�
,
�
(
�
,
�
)
)
{\displaystyle \scriptstyle (X,Y)\sim ({\mathcal {X}}\times {\mathcal {Y}},p(x,y))} definujeme jako relativní entropii
�
(
�
(
�
,
�
)
‖
�
(
�
)
�
(
�
)
)
{\displaystyle \scriptstyle D(p(x,y)\|p(x)p(y))} mezi skutečným rozdělením p(x,y) dvojice zpráv (X,Y) a součinovým rozdělením q(x,y) = p(x)p(y), kterým by se dvojice řídila, kdyby její komponenty X a Y byly vzájemně nezávislé, tj.

�
(
�
;
�
)
=
∑
�
∈
�
∑
�
∈
�
�
(
�
,
�
)
log
⁡
�
(
�
,
�
)
�
(
�
)
�
(
�
)
.
{\displaystyle I(X;Y)=\sum _{x\in {\mathcal {X}}}\sum _{y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x,y)}{p(x)p(y)}}.}
(Logaritmus se nejčastěji uvažuje dvojkový. V tom případě je informace vyjádřena v bitech.)


Průměrná vzájemná informace definovaná prostřednictvím (marginální) entropie, podmíněné entropie a sdružené entropie [18], dvou náhodných proměnných X, Y [19]:

I(X;Y) = H(X) - H(X/Y)
I(X;Y) = H(Y) - H(Y/X)
I(X;Y) = H(X) + H(Y) - H(X, Y)
I(X;Y) = H (X, Y) - H(X/Y) - H(Y/X),
kde H ( X ) a H ( Y ) jsou (marginální) entropie, H ( X | Y ) a H ( Y | X ) jsou podmíněné entropie a H ( X , Y ) je sdružené entropie proměnných X a Y .

V definičním vztahu pro vzájemnou informaci udává entropie H(X) míru nejistoty hodnoty náhodné proměnné X, a H(X|Y) je množství zbývající nejistoty o X, když je Y známé. Rozdíl H(X) - H(X/Y) udává množství nejistoty v X, redukované o množství nejistoty v X, která zbývá, za podmínky, že je Y známé. Z tohoto se ukazuje význam vzájemné informace jako množství informace (tj. snížení nejistoty), které znalost jedné proměnná poskytuje o druhé.

Nastávají dva extrémní případy:

První extrém nastává, pokud jsou náhodné proměnné X a Y nezávislé (neurčitost vzájemné vazby je nekonečná), znalost jedné nevypovídá nic o druhé, jevy z Y nepřispívají žádnou informací, neurčitost H(X/Y) není ovlivněna hodnotami Y, tak H(X/Y) = H(X)) a vzájemná informace I(X; Y) proměnných X a Y je:

I(X;Y) = H(X) - H(X/Y) = H(X) - H(X) = 0
Druhým extrémem je situace, kdy mezi náhodnými proměnnými X a Y je neurčitost závislosti nulová, a tak je závislost deterministická. Pak proměnné X a Y jsou funkčně závislé Y = f (X), kde f je deterministická funkce. Tak každá z oněch proměnných obsahuje úplnou znalost o té druhé, znalost Y určuje X, a informace obsažená v rozdělení pravděpodobnosti jedné z proměnných je stejná, jako informace obsažená v pravděpodobnostním rozdělení té druhé. V deterministické závislosti mezi proměnnými X, Y je nulová neurčitost, a tak podmíněná entropie H(X/Y) = H(X/X) = 0 (pravděpodobnostní rozložení proměnných X a Y jsou stejná, nesou stejnou neurčitost, tak i informaci) a H(X/Y) nepřispívá ke snížení neurčitosti H(X), a vzájemná informace podle definičního vztahu je:

I(X;Y) = H(X) - H(X/Y) = H(X) – 0 = H(X)
Mezi reálným světem a informací
Prostředníkem mezi reálným světem a informací o něm je poznání viz Věda. Při umělém exaktním poznání je možno zřetelně vidět krok poznání od materiálního světa k informaci o něm. Slouží k tomu vědou rozpoznaný, v různých oborech patřičně formulovaný zákon akce a reakce, někdy poněkud skrytý v použitém matematickém nástroji. Umožňuje z materiálního světa interakcí, vytvořit model, v němž jsou interakce rozpleteny a tak osamostatněny na jednostranně působících akce a reakce. V okamžiku osamostatnění (je to přelom z reálného světa do znalostního modelu – získané informace) se otevírá přístup k informaci. V popisu reálného světa exaktní vědou, je získaná informace v podobě veličin (fyzikální, chemické, biologické ...) svázaných matematickými vztahy, popisujícími přírodní zákony, tedy v podobě matematického či počítačového modelu daného objektu nebo jevu. K zobrazení rozpletených interakcí názorně slouží často používané orientované grafy, nazývané bloková schémata,[20] či grafy signálových toků.[21][22] Rozpletené interakce se v grafové reprezentaci zobrazují jako jednosměrné akce a reakce tvořící zpětnovazební smyčky.

V přirozeném poznání neodstranitelné mlhy vágnosti, tento zvrat v poznání (od reality k informaci) již tak názorný není.

Informace a interpretace
Vidění reálného světa jako světa interakcí, je obecný model jeho vidění současnou vědou (např.[23]). Vidění toku informace v něm, je speciální pohled, vytvořený při studiu principů sdělovacích soustav a rozvinutý v kybernetice. Způsob jiného vidění spočívá v tom, že se pozoruje něco, co plyne odněkud někam (třeba jako nějaká vlna na vodní hladině, v níž se skrývá mnoho interakcí, které se však nevnímají), tedy jednosměrně, což u interakcí neplatí. Takový pohled je v některých případech vhodnější pro pochopení dění, vidění jiných souvislostí. Názorně k tomu slouží výše zmíněné orientované grafy: bloková schémata a grafy signálových toků. Tyto grafy vytvořené přepisem matematického modelu do grafického schématu, svojí názorností přímo nabízejí použít vhodnější z obou interpretací.

Obou interpretací se již dlouhý čas vědomě používá v návrhu soustav sdělovacích a soustav automatického řízení[24][25] v teorii elektrických obvodů, ale někdy i v počítačové simulaci různých jevů materiálního světa. Informace je jen jiný pohled na materiální dění, jiná interpretace. Ustavila se na ní i kybernetika,[26] těžící i z toho, že tato interpretace je použitelná napříč obory. Třeba interakci čichového orgánu brouka s molekulami feromonu, můžeme chápat jako informaci, je to ale interakce. Podobně můžeme říci, že genetický kód je informací pro vývoj buňky, přitom je jedním článkem z řetězce biochemických interakcí. Je možno ale uplatnit (modelovou) interpretaci, že je nositelem informace, pokud je to výhodnější pro pochopení jevu. Vhodně strukturované elektromagnetické pole, detekované a zpracované mobilním telefonem můžeme interpretovat jako SMS zprávu, nebo jako řetězec interakcí. Člověk má ve svém těle řadu receptorů, a tak jeho centrální nervová soustava může zpracovávat i elektrochemické a biochemické veličiny vypovídající o jeho tělesných stavech – pocitech z nich, což může chápat jako informaci.

Tak i entropie, jako každý aplikovaný matematický vztah, může mít řadu různých interpretací. Není proto vhodné apriori svazovat interpretaci entropie v informačním pojetí s fyzikální interpretací. To by mělo smysl, pokud by se hledala nějaké analogie mezi nimi, a plynuly z toho nové poznatky. Snaha o tuto vazbu vznikla spíš jako ozvěna historie Shannonovy práce, ústící ve vztah entropie, se stejnou matematickou formou, k níž došla termodynamika. Bez bližšího upozornění může vznikat klamný dojem, že informace není pouze lidský uchopovací nástroj a náhled na model dění v reálném světě, ale aktér působící v něm místo interakce. Zde je vhodné připomenout, že v době, kdy Shannon svoji hypotézu matematicky formuloval, existoval jediný matematický nástroj reprezentace neurčitosti, a to byla stochastika, kterou použil. Tedy jako poznámka: Dnes, v r. 2021, matematika nabízí i jiné reprezentace jako fuzzy množiny a fuzzy logiku, a kandidátem mohou být i hrubé množiny, a další nástroje. Pokud by Shannon použil jinou variantu reprezentace neurčitosti, byl by získaný vztah jiný – formálně jiný jazykový útvar – matematický vztah. Do jaké míry by Shannonova entropie a vztah získaný jinou variantou matematické reprezentace neurčitosti byly ekvivalentní, nebo který typ reprezentace je vhodnější, by se pak mohlo uvažovat.

Je nutno pečlivě rozlišovat, kdy mluvíme o reálném světě interakcí, a kdy o jeho kognitivním modelu, a kdy máme na mysli jakou interpretaci. Jinak vznikají myšlenkové mišmaše (vedoucí k nedorozuměním), občas doprovázené grafickými mišmaši.

Jednou z podivných myšlenek je Capurrovo trilema, kdy se autor pídí po podstatě informace, ale rozděluje informaci na tři typy podle materiální realizace jazyka, a podle vztahu k člověku. Zda ji člověk používá ke komunikaci nebo zda je to informace mimo lidskou komunikaci, která zajišťuje běh reálného světa, např. informace z genetického kódu řídící vývoj buňky. Pokud by si všiml možných interpretací, ve výše zmíněných oborech, snadno by se zorientoval.

Informace a uspořádanost
Informace a uspořádanost je téma, se kterým se lze občas setkat v literatuře.

Mezi uspořádaností (organizovaností, strukturou, řádem) a informací je dvoustranný vztah.

První je, že existuje struktura (jsme schopni ji rozpoznat), kterou umíme interpretovat (přiřadit význam), a tak získat informaci. V tomto případě struktura slouží jako jazyk, např. dendrologický řez, geologické vrstvy odkryté při těžbě kamene či písku, viditelné ve vzniklé stěně, rentgenový snímek lidského orgánu, např. plic, kde odborník dovede rozpoznat zdravou strukturu a strukturu napadenou infekcí, řád hledaný v elektromagnetickém záření zachyceném radioteleskopem, tištěný či v kameni vytesaný text.

Druhý je, kdy na základě informace (předpisu, projektu), má být vytvořena nějaká uspořádanost, struktura či řád v reálném světě, a tak se tam má provést požadovaná změna. To, jak už bylo výše řečeno (v odst. Informace a jazyk), znamená, že musíme zajistit takové působení interakcí v reálném světě, aby projektované dílo vzniklo. Takových příkladů je mnoho, tváření a obrábění kovů dle výkresové dokumentace, stavba počítačů, stavba domů, vyšívání vzorů látek dle zadaného předpisu, lisování gramofonových desek, ale i automatické řízení, kde asi nejznámějším je Wattův regulátor otáček. V posledním příkladu je řádem to, že otáčky parního stroje mají požadovanou (obsluha ji nastavuje) hodnotu, která se téměř nemění se zatížením stroje, či s momentálním výkonem kotle (tlakem páry) viz Zpětná vazba.

Občas se v literatuře objevuje myšlenka spojovat definici informace s pojmem uspořádanosti či organizace. V diplomové práci Filipa Zlámala[8] je názorný příklad, že stavět definici pojmu informace na pojmu uspořádanosti či organizace je chybné, neboť pojem uspořádanosti není objektivně definovatelný. Tento pramen a rovněž další prameny[27][28] umožňují, hlouběji nahlédnout do problematiky entropie a její interpretace.

Zpracování informace
Z jistého množství poznáním získané informace, lze za jistých podmínek, získat další informaci, která je v té prvotně získané, člověku skryta. Postupu se říká usuzování, a má svoji přirozenou formu (informace zpracovávaná lidskou psýchou je inherentně vágní), známou z usuzování pověstného Sherlocka Holmese, a svoji formu exaktní (formální, možnou pouze pro informaci s nulovou vnitřní vágností) používanou v matematice, a známou jako inference. Inference[29][30] znamená z výchozích jazykových konstruktů (axiomů) získání (podle daných pravidel, např. dovolených úprav rovnic) nových jazykových konstruktů, které v jisté interpretaci mohou přinášet novou informaci. Daná soustava informace může poskytnout jen určité množství usuzováním získané informace, pro další informaci je nutno vrátit se k reálnému světu, uskutečnit další poznání, a soustavu znalostí tak rozšířit.[31] Buď tak získáme hledanou informaci, či v rozšířené soustavě znalostí ji opět hledáme na základě inference.

Zpracováním informace lze získávat i informaci nad danou soustavou informace, třeba nelezení jistého řádu v soustavě. Takto získané informaci se říká metainformace. Zpracováním informace lze předpovídat zatím nepoznané jevy, předpovídat zatím nepoznaná data, a tak je nabízet k experimentálnímu ověření či zamítnutí. Modifikaci (inferenci) jazykových konstruktů (viz Exaktní věda, tam příklad inference) formálního jazyka (matematika, formální logika, programovací jazyky) může podle daných pravidel provádět stroj s diskrétními stavy (Turingův stroj, počítač).[29][32] V tom případě stavům stroje vhodně přiřadíme konstrukty onoho jazyka, a sled stavů stroje řídíme podle daných pravidel. Tak lze proces inference automatizovat, ovšem inference vyžaduje metainformaci, o tom, jak řadit inferenční kroky, což je lidská znalost, a tu je nutno stroji sdělit – naprogramovat. Formální inferenci lze provést pouze pro informaci s nulovou vnitřní vágností, neboť jen tu lze reprezentovat formálním jazykem a zpracovat formální inferencí, ať již strojem či tužkou na papíru. Interpretaci inferencí získaných jazykových konstruktů musí provést člověk.

Přenos informace – komunikace
Princip lidské komunikace
Lidská komunikace je proces, který se dá schematicky popsat z hlediska přenosu a zpracování informace. Popisuje se princip, kterým člověka obdařila příroda. Je to model postavený na základních pojmech, které jej orientují ke kognitivní psychologii a informatice. Podrobnosti jsou uvedeny na Lidská komunikace – princip.

Technické prostředky komunikace
S rozvojem elektrických a elektronických komunikací se informace stává i technickým pojmem. Zájem se soustřeďuje na formální jazykovou stránku, a nezabývá se jejím smyslem či obsahem. Základním modelem pro přenos informace je soustava vysílač (kodér) – kanál – přijímač (dekodér). Roku 1948 publikoval Claude Shannon, který pracoval pro Bellovy laboratoře, průkopnickou publikaci A Mathematical Theory of Communication,[33] v níž se soustřeďuje na přenos zpráv, kódovaných v nějaké abecedě o konečném nebo spočetném množství znaků. Tak se stal jedním ze zakladatelů teorie informace.

Výklad pojmu informace
Ve starší odborné literatuře můžeme najít snahu o uchopení pojmu informace rozdělením vnitřního obsahu pojmu do několika kategorií. (Niedhardt P.: Einführung in die Informationstheorie. Verlag Technik Berlin, Berliner Union Stuttgart 1957)

Informace sémantická se zabývá pouze sémantickým významem slov.
Informace pragmatická zohledňuje pouze přírůstek znalostí. Již známé není informací v tomto slova smyslu. Škola je zdrojem takovýchto informací.
Informace idealizovaná je dána individuálním hodnocením příjemce a je závislá na jeho předchozím vzdělání a zkušenostech, ale i na jeho okamžitém emocionálním stavu. Např. co se mi ráno líbilo, už nemusí platit odpoledne.
Informace inženýrská, definovaná C. E. Shannonem jako "snížení neurčitosti systému" a matematicky vyjádřená jako logaritmus pravděpodobnosti nějakého jevu (přenosu zprávy) při rovnoměrném rozložení hustoty pravděpodobnosti. Pro pravděpodobnost 1/2 při dvojkovém logaritmu dostaneme jednotku zvanou bit. Bit je pouze jednotka informace, jako je metr nebo coul jednotkou vzdálenosti.
Vlastnosti informace
Informace musí mít určité vlastnosti

Informace by měla být:

pravdivá
srozumitelná – různé jazyky, různé kódy, šifrování,
včasná,
relevantní, česky souvztažná, významná v dané souvislosti – ne „já o koze, ty o voze“,
etická – platí jen pro mezilidské vztahy. Není to podmínka nutná, jako předchozí, ale žádoucí.
Informace je nezávislá na svém energetickém nosiči. Změna energetického nosiče má za následek pouze změnu vlastností přenosové trasy (kanálu), ale smysl či obsah informace nemění. Je to jako s přepravou nákladu – náklad se nemění, ať použijeme k přepravě koně, auto, nebo letadlo.

Některé souvislosti
Signál je vysílaná jazyková struktura, do níž je vtisknuta informace. V případě příjmu, (případném dekódování), a interpretaci recipientem se pro něho stává informcí. Signál je stěžejním pojmem informatiky.
Modulace' je metoda vytvoření signálu na jeho nosiči, tj. vytvoření (jazykové) struktury nesoucí informaci a umožňující materializaci jazyka zejména v případě, kdy je pro materializaci použito elektromagnetického pole. Příklady použití jsou rozhlas, televize, bezdrátová pojítka, radiolokace.
Šum je rozdíl mezi tím, co bylo do informačního kanálu vloženo a tím, co bylo přijato. U informace se rozlišuje syntaktický a sémantický šum. Shannonovská informatika uvažuje pouze syntaktický šum, což je jakékoli poškození jazykové struktury (formy) nesoucí informaci, které znesnadňuje rozpoznání (čtení) té struktury, a tak zapříčiňuje chyby jejího čtení. Příkladem je hluk v místnosti, který ruší srozumitelnost rozhovoru, šumění při příjmu rádiového vysílání, „zasněžený“ či potrhaný obraz na monitoru televizoru, nedotisknuté (chybí části písmen) nebo zašpiněné písmo na papíru, potrhaný či poškrabaný obraz na plátně atp.
Kanál je informační cesta, Shannonovo schéma toku informace od zdroje k adresátovi, kde se uvažuje působení šumu, a pak jakákoli jeho realizace – např. i poštovní holub může být součástí informačního kanálu.
Záměrně falešná „informace“ se nazývá dezinformace nebo lež, pracuje s ní například propaganda. Rozdíl mezi dezinformací a lží spočívá v tom, že lež je používána pasivně jako obrana, zatímco dezinformace je aktivní. Nezáměrně špatná informace je chyba nebo omyl. Dezinformace, stejně jako chyba či omyl vede k nárůstu entropie systému, zatím co informace vede k poklesu entropie. Živé organizmy mají zápornou entropii.
Informace souvisí s rozlišováním, s rozdílem a určeností (například jako černé písmeno na bílém papíře). Informace má význam a je ji třeba odlišit od jejího hmotného nosiče – ať je to hlas, zvuk, obraz, písmo nebo disk. Povahu informace dobře vystihuje definice amerického antropologa Gregory Batesona: informace podle něho znamená „takový rozdíl, na němž záleží“ (the difference which makes difference).[34]

Schopnost rozlišení informace (srozumitelnosti) pak souvisí s tzv. odstupem signálu od šumu, který se udává v decibelech jako 10 × logaritmus poměru energie šumu k energii signálu.

Šíření a uchovávání informace
Rostoucí význam informací ve smyslu vědomostí a zpráv obrátil od konce 19. století pozornost vědců – zejména lingvistů – ke studiu znakových systémů a kódů. Informace se šíří řečí a v historické době i písmem, které ji kódují pomocí slov, hlásek a písmen. Ty nesou informaci jen proto, že tvoří ucelené systémy (slovník, fonetiku, abecedu): hlásky nesou význam jen potud, pokud se od sebe zřetelně liší (Ferdinand de Saussure). S písmem vznikla první možnost trvalého uchovávání informace na hmotném nosiči, jež se dále rozvinula knihtiskem a konečně i elektronickými způsoby uchovávání kódované informace. Odtud vznikl i pojem informační společnost – společnost, která se ve stále větší míře opírá o shromažďování, využívání a šíření informací.[35]

Zpracování informací
Zatímco účelem analogového přenosu informace je pouze přeměna hlasu nebo obrazu na elektrický signál, jeho věrný (nezkreslený) přenos a opačná změna na zvuk nebo obraz, už v telegrafii se objevila možnost úsporného kódování znaků. Morseova abeceda kóduje písmena a číslice do posloupností krátkých a dlouhých signálů (tečka a čárka), které se snadno a spolehlivě přenášejí. Na podobném základě pracuje i dálnopis, kde se však převod znaků abecedy do binárního tvaru (a nazpět) děje automaticky.

Kódovaná informace a její nosiče (děrná páska, děrný štítek, magnetické nosiče atd.) otevřela i další možnosti strojového zpracování informace, z nichž se vyvinula současná počítačová technika a informatika. Slovo „informace“ se zde však používá v několika rovinách: text, který právě píši, kóduji do písmen, ta se v počítači převádějí na binární kódy (ASCII, UNICODE) a přenášejí mezi počítači v určitých formátech, ale nakonec jako posloupnosti elektrických signálů, případně optických či elektromagnetických. Informace, které do systému vkládáme, dospějí ke svým adresátům jen tehdy, pokud všechny tyto transformace jejich kódování a zobrazení proběhnou podle přesně stejných pravidel v přijímači jako ve vysílači, jen v opačném směru. Přenosové sítě proto musejí pracovat s víceúrovňovými modely, kde si vysílače a přijímače na jednotlivých úrovních „rozumějí“: pisatel se čtenářem, program s programem, počítač s počítačem, modem s modemem atd.

Zpracováním informací obecněji se rozumí nejen přenos, ale i transformace informací. V počítačích a spol. se informace reprezentují pomocí dat.

Měření informace
V některých oborech lidské činnosti se informace měří prostřednictvím jazykových konstruktů (znaků, slov, vět, nebo knižních stránek...). Na poště se telegramy hodnotily podle počtu slov, v knihovnictví se užívá měření na stránky, novináři počítají slova nebo znaky a v počítači se informace měří na bajty nebo bity. Jeden bit, odpovídá jednomu rozlišení typu „ano – ne“, nese informaci, ze které se dozvíme, který stav nastal ze dvou stavů stejně pravděpodobných (např. házení mincí). Je to základní jednotka informace. Bajt (anglicky byte) je skupina zpravidla osmi bitů, tím pádem může nabývat 256 různých hodnot a zhruba tak odpovídá jednomu znaku abecedy evropského jazyka (asijské znaky bývají obvykle kódovány užitím 2 až 4 bajty). Je to ovšem množství informace pouze v technickém smyslu slova: stránka textu může mít 1800 bajtů, to ale neříká nic o tom, co je v nich zaznamenáno. Stejnou velikost mohou mít užitečné a smysluplné zprávy, prázdné řeči nebo konečně i samé mezery. Pro stanovení informační hodnoty souboru dat lze použít entropii.

Informace v dalších vědách
Vedle běžného významu informací jako zpráv a vědomostí se informace dnes stala odborným pojmem v biologii i ve společenských vědách, zejména pod vlivem objevu genomu a genetického kódu. Zde je často ztotožňována informace s jejím původem, spočívajícím ve vlastnostech a uspořádání pozorovaných objektů. Genetická informace je totiž kódována pomocí pouhých čtyř nukleotidů (ACTG), jejichž trojice (kodóny) řídí syntézu proteinů; běžnými prostředky lze tedy měřit její informační obsah.

Podobný informačně teoretický pohled se dnes rozvíjí také ve fyzice i ve společenských vědách.

Odkazy
Reference
ikona
Tento seznam referencí potřebuje upravit.
Tento článek obsahuje reference, které nemají standardní formu. Přidejte prosím doplňující informace a upravte je do podoby standardních citací. Pomůžete tím udržet ověřitelnost a jednotný vzhled a styl článků.
 JONÁK, Zdeněk. Informace. In KTD: Česká terminologická databáze knihovnictví a informační vědy (TDKIV) [online]. Praha: Národní knihovna ČR, 2003 [cit. 2011-08-07]. Dostupné z: http://aleph.nkp.cz/F/?func=direct&doc_number=000000456&local_base=KTD.
 Mareš M. Zdroje informací a jejich měření. Ústav aplikované informatiky. Přírodovědecká fakulta, Jihočeské univerzity v Českých Budějovicích, 2018. https://docplayer.cz/161264417-Zdroje-informace-a-jeji-mereni.html
 Matoušek R.: Teorie informace, http://www.uai.fme.vutbr.cz/~matousek/TIK/dokumenty/osmera_kap2.pdf Archivováno 24. 4. 2021 na Wayback Machine.
 Chomsky, N: Aspects of the Theory of Syntax. Cambridge, Mass.: M.I.T. Press, 1965.
 Chomsky, N: Current Issues in Linguistic Theory. De Gruyter Mouton; Printing 1975
 Charles W. Morris: Foundations of the Theory of Signs. International Encyclopedia of Unified Science 1 (2). Chicago: University of Chicago Press. 1946
 Abramson Norman: Information Theory and Coding. McGraw-Hill Education, 1963. ISBN 0-070-00145-6. https://dokumen.tips/documents/information-theory-and-coding-by-norman-abramson.html?page=32
 Filip Zlámal: Entropie. MASARYKOVA UNIVERZITA, Přírodovědecká fakulta, Ústav matematiky a statistiky. Brno 2016. https://is.muni.cz/th/aiz9q/diplomova_prace_Zlamal_Filip.pdf
 Abramson, Norman: Information Theory and Coding. McGraw-Hill Education, 1963. ISBN 0-070-00145-6. ISBN 0-070-00145-6. https://dokumen.tips/documents/information-theory-and-coding-by-norman-abramson.html?page=32
 Hartley, R.V.L., "Transmission of Information", Bell System Technical Journal, Volume 7, Number 3, pp. 535–563, (July 1928).
 C. E. Shannon, The Mathematical Theory of Communication, Bell Syst. Techn. J. 27 (1948), č. 3, str. 379–423. http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf Archivováno 15. 7. 1998 na Wayback Machine.
 Filip Zlámal: Entropie. Masarykova universita Brno, Přírodovědecká fakulta, Ústav matematiky a statistiky, 2016. https://is.muni.cz/th/aiz9q/diplomova_prace_Zlamal_Filip.pdf?studium=220622
 Capurro Rafael, Hjrarland Birger: The Concept of Information. Annual Review of lnformation Science and Technology 2002. http://www.capurro.de/Capurro_Hjoerland.pdf
 Mareš M. Zdroje informací a jejich měření. Ústav aplikované informatiky. Přírodovědecká fakulta, Jihočeské univerzity v Českých Budějovicích, 2018, str. 51, 54.
 Shannon, Claude Elwood (1948): A Mathematical Theory of Communication. Bell System Technical Journal . 27 (3), pp. 379–423.[1] Archivováno 31. 1. 1998 na Wayback Machine.
 Kreer, J. G. (1957). "A question of terminology". IRE Transactions on Information Theory. 3 (3): 208.
 Vajda I., Teorie informace. Vydavatelství ČVUT, Praha 2004. ISBN 80-01-02986-7
 Navara Mirko: Teorie informace. https://cmp.felk.cvut.cz/~navara/psi/TI_ebook.pdf
 Cover, T.M.; Thomas, J.A. (1991). Elements of Information Theory (Wiley ed.). John Wiley & Sons. ISBN 978-0-471-24195-9.
 Zítek P.: Simulace dynamických systémů. SNTL Praha 1990
 Mason, S.J.: Feedback Theory: Further Properties of Signal Flow Graphs. Proc. IRE, Vol. 44, No. 7, pp. 920-926, 1956.
 Biolek D.: Grafy signálových toků pro analýzu obvodů (nejen) v proudovém módu. http://www.elektrorevue.cz/clanky/02031/index.html#1 Archivováno 24. 1. 2020 na Wayback Machine.
 Petr Kulhánek: Mikrosvět a makrosvět. https://www.youtube.com/watch?v=HQgIZl9TpM4
 Švec J., Kotek Z.: „Teorie automatického řízení“. SNTL, Praha, 1969
 Balátě J.: „Automatické řízení“. BEN – technická literatura. 2004.
 Mareš M.: Dvojité výročí kybernetiky, Vesmír 88, 270, 2009/4, https://vesmir.cz/cz/casopis/archiv-casopisu/2009/cislo-4/dvojite-vyroci-kybernetiky.html
 Arieh Ben-Naim: ENTROPY: The Greatest Blunder in the History of science, Kindle Edition, 2021
 Arieh Ben-Naim: Entropy Demystified – The Second Law Reduced to Plain Common Sense, 2nd Edition, World Scientific 2016
 Havel, I. M., Hájek, P. Filozofické aspekty strojového myšlení. In Sborník SOFSEM'82, 1982, str. 171–211
 Křemen, J.: „Nový pohled na možnosti automatizovaného (počítačového) odvozování“. Slaboproudý obzor. Roč. 68 (2013), č. 1., str. 7 – 11., https://docplayer.cz/4300687-Novy-pohled-na-moznosti-automatizovaneho-pocitacoveho-odvozovani.html
 Mareš M.: Zdroje informací a jejich měření. Ústav aplikované informatiky. Přírodovědecká fakulta, Jihočeské univerzity v Českých Budějovicích, 2018. https://docplayer.cz/161264417-Zdroje-informace-a-jeji-mereni.html
 Hopcroft,J.E, Motwani, R., Ullman, J.D: Introduction to Automata Theory, Languages, and Computation, 3rd Edition, Addison-Wesley, 2006.
 Shannon, Claude Elwood (1948): A Mathematical Theory of Communication. Bell System Technical Journal . 27 (3), pp. 379–423.[2] Archivováno 31. 1. 1998 na Wayback Machine.
 G. Bateson, Steps to an ecology of mind. St. Albans 1973, str. 428.
 Sokol, Filosofická antropologie. Oddíl „Společenská komunikace“, str. 77–80.
Literatura
Bawden, D., Robinson, L., Úvod do informační vědy. Přel. M. Lorenz, K. Mikulášek, D. Vévodová. Doubravník: Flow 2017. ISBN 978-80-88123-10-1
Beneš P., Informace o informaci. BEN - technická literatura, Praha 2010, 123 s. ISBN 978-80-7300-263-3
J. Cejpek, Informace, komunikace a myšlení: úvod do informační vědy. Praha: Karolinum, 1998 – 179 s. ISBN 80-7184-767-4
Niedhardt P., Einführung in die Informationstheorie. Verlag Technik Berlin, Berliner Union Stuttgart 1957
J. Sokol, Filosofická antropologie. Praha: Portál 2003
Stonier T., Informace a vnitřní struktura vesmíru. BEN - technická literatura, Praha 2002, 150 s., ISBN 80-7300-050-4
Zlámal F., Entropie. MASARYKOVA UNIVERZITA, Přírodovědecká fakulta, Ústav matematiky a statistiky. Brno 2016. https://is.muni.cz/th/aiz9q/diplomova_prace_Zlamal_Filip.pdf
Související články
Entropie
Informatika
Věda
Jazyk (lingvistika)
Vágnost
Poznatek
Zpráva
Externí odkazy
Logo Wikimedia Commons Obrázky, zvuky či videa k tématu informace na Wikimedia Commons
 Slovníkové heslo informace ve Wikislovníku
 Téma Informace ve Wikicitátech
Informace v České terminologické databázi knihovnictví a informační vědy (TDKIV)
Kurz práce s informacemi [online]. Masarykova univerzita, 2007-05-10. Dostupné online.
KUŽELÍKOVÁ, NEKUDA, POLÁČEK. Sociálně-ekonomické informace a práce s nimi [online]. Masarykova univerzita, 2008-04-26. Dostupné online.
Stanford encyclopedia of philosophy, heslo Semantic Conceptions of Information (anglicky)
Prof. Milan Mareš: Zdroje informace a jejich měření Archivováno 11. 10. 2019 na Wayback Machine.
Autoritní data Editovat na Wikidatech
NKC: ph121095PSH: 1713TDKIV: 000000456BNE: XX526504GND: 4026899-8
Portály: Filozofie | Informační věda a knihovnictví
Kategorie: InformaceFilozofie jazyka
Stránka byla naposledy editována 9. 4. 2024 v 10:04.
Text je dostupný pod licencí Creative Commons Uveďte původ – Zachovejte licenci, případně za dalších podmínek. Podrobnosti naleznete na stránce Podmínky užití.
Ochrana osobních údajůO WikipediiVyloučení odpovědnostiKontaktujte WikipediiKodex chováníVývojářiStatistikyProhlášení o cookiesMobilní verzeWikimedia FoundationPowered by MediaWiki
Přepnout omezenou šířku obsahu